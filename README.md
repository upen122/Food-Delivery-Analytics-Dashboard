<!-- Project Banner -->
<p align="center">
  <img src="https://img.shields.io/badge/Big%20Data-PySpark-orange?style=for-the-badge" />
  <img src="https://img.shields.io/badge/Cloud-AWS_S3-yellow?style=for-the-badge" />
  <img src="https://img.shields.io/badge/Dashboard-Plotly_Dash-blue?style=for-the-badge" />
  <img src="https://img.shields.io/badge/Status-Completed-brightgreen?style=for-the-badge" />
</p>

<h1 align="center">ğŸ“Š Food Delivery Analytics Dashboard</h1>

<p align="center">
  <em>A Big Data pipeline project that extracts, transforms, and visualizes real-world food delivery metrics using PySpark, AWS, and Dash.</em>
</p>

---

## ğŸ“š Table of Contents

- [ğŸ” Project Overview](#-project-overview)
- [ğŸ“ Dataset Information](#-dataset-information)
- [ğŸ§° Tech Stack](#-tech-stack)
- [âœ¨ Features](#-features)
- [ğŸš€ How to Run](#-how-to-run)
- [ğŸ“¸ Dashboard Preview](#-dashboard-preview)
- [ğŸ“¤ Upload to AWS S3](#-upload-to-aws-s3)
- [ğŸŒ Launch Dashboard using ngrok](#-launch-dashboard-using-ngrok)
- [ğŸ§  What I Learned](#-what-i-learned)
- [ğŸ§‘â€ğŸ’» Author](#-author)
- [ğŸ”— Connect with Me](#-connect-with-me)

---

## ğŸ” Project Overview

This project builds a complete **end-to-end Data Engineering & Visualization pipeline** using a real-world food delivery dataset. It leverages **PySpark** for scalable processing, stores cleaned data on **AWS S3**, and builds a **dynamic interactive dashboard** using **Plotly Dash**.

ğŸ¯ **Goal:** Derive operational insights to improve delivery performance, analyze delivery times, and understand the impact of weather, traffic, and festivals on customer experience.

---

## ğŸ“ Dataset Information

- ğŸ“¦ Source: Kaggle Food Delivery Dataset  
- ğŸ¯ Contains attributes like:
  - Delivery ratings, time, weather, traffic conditions
  - Rider details (age, experience, vehicle condition)
  - Festival and multi-order indicators
  - City zones, restaurant, and delivery locations

---

## ğŸ§° Tech Stack

| Layer               | Technology             |
|--------------------|------------------------|
| Language           | Python 3.x             |
| Big Data Processing| PySpark (Google Colab) |
| Cloud Storage      | AWS S3 (via `boto3`)   |
| Visualization      | Plotly Dash            |
| Notebook           | Jupyter (Colab)        |
| Version Control    | Git + GitHub           |

---

## âœ¨ Features

- âœ… **ETL Pipeline** using PySpark DataFrames
- âœ… **Null & Duplicate Handling**, Data Cleaning, Type Casting
- âœ… **AWS S3 Integration** for cloud storage of cleaned data
- âœ… **Real-Time Dashboard** with interactive graphs
- âœ… **Insights on**:
  - Impact of Weather/Traffic
  - Festival-time Delivery Trends
  - Rider Ratings vs Delivery Timings
  - Multi-order Effects

---

## ğŸ“¤ Upload to AWS S3

import boto3

# Initialize S3 client
s3 = boto3.client('s3', aws_access_key_id='YOUR_ACCESS_KEY',
                        aws_secret_access_key='YOUR_SECRET_KEY')

# Upload the file
s3.upload_file('cleaned_data.csv', 'your-bucket-name', 'cleaned_data/cleaned_data.csv')

print("File uploaded to S3 successfully.")

---

## ğŸŒ Launch Dashboard using ngrok

Step 1: Install pyngrok
bash
Copy
Edit
pip install pyngrok
Step 2: Run ngrok tunnel
python
Copy
Edit
from pyngrok import ngrok

# Open tunnel on port 8050 (Dash default)
public_url = ngrok.connect(8050)
print(f"Public URL: {public_url}")
Step 3: Run your Dash app in another terminal
bash
Copy
Edit
python app.py
