<!-- Project Banner -->
<p align="center">
  <img src="https://img.shields.io/badge/Big%20Data-PySpark-orange?style=for-the-badge" />
  <img src="https://img.shields.io/badge/Cloud-AWS_S3-yellow?style=for-the-badge" />
  <img src="https://img.shields.io/badge/Dashboard-Plotly_Dash-blue?style=for-the-badge" />
  <img src="https://img.shields.io/badge/Status-Completed-brightgreen?style=for-the-badge" />
</p>

<h1 align="center">ðŸ“Š Food Delivery Analytics Dashboard</h1>

<p align="center">
  <em>A Big Data pipeline project that extracts, transforms, and visualizes real-world food delivery metrics using PySpark, AWS, and Dash.</em>
</p>

---

## ðŸ“š Table of Contents

- [ðŸ” Project Overview](#-project-overview)
- [ðŸ“ Dataset Information](#-dataset-information)
- [ðŸ§° Tech Stack](#-tech-stack)
- [âœ¨ Features](#-features)
- [ðŸ“Œ Pipeline Architecture](#-pipeline-architecture)
- [ðŸš€ How to Run](#-how-to-run)
- [ðŸ“¸ Dashboard Preview](#-dashboard-preview)
- [ðŸ§  What I Learned](#-what-i-learned)
- [ðŸ”— Connect with Me](#-connect-with-me)

---

## ðŸ” Project Overview

This project builds a complete **end-to-end Data Engineering & Visualization pipeline** using a real-world food delivery dataset. It leverages **PySpark** for scalable processing, stores cleaned data on **AWS S3**, and builds a **dynamic interactive dashboard** using **Plotly Dash**.

ðŸŽ¯ **Goal:** Derive operational insights to improve delivery performance, analyze delivery times, and understand the impact of weather, traffic, and festivals on customer experience.

---

## ðŸ“ Dataset Information

- ðŸ“¦ Source: Kaggle Food Delivery Dataset  
- ðŸŽ¯ Contains attributes like:
  - Delivery ratings, time, weather, traffic conditions
  - Rider details (age, experience, vehicle condition)
  - Festival and multi-order indicators
  - City zones, restaurant, and delivery locations

---

## ðŸ§° Tech Stack

| Layer               | Technology             |
|--------------------|------------------------|
| Language           | Python 3.x             |
| Big Data Processing| PySpark (Google Colab) |
| Cloud Storage      | AWS S3 (via `boto3`)   |
| Visualization      | Plotly Dash            |
| Notebook           | Jupyter (Colab)        |
| Version Control    | Git + GitHub           |

---

## âœ¨ Features

- âœ… **ETL Pipeline** using PySpark DataFrames
- âœ… **Null & Duplicate Handling**, Data Cleaning, Type Casting
- âœ… **AWS S3 Integration** for cloud storage of cleaned data
- âœ… **Real-Time Dashboard** with interactive graphs
- âœ… **Insights on**:
  - Impact of Weather/Traffic
  - Festival-time Delivery Trends
  - Rider Ratings vs Delivery Timings
  - Multi-order Effects

---

## ðŸ“Œ Pipeline Architecture

```mermaid
flowchart TD
    A[Raw Dataset (CSV)] --> B[PySpark ETL - Google Colab]
    B --> C[Cleaned DataFrame]
    C --> D[Upload to AWS S3]
    C --> E[Plotly Dash Dashboard]
    D --> F[Analytics via Cloud Data]

