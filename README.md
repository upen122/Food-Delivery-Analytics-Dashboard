# ğŸš€ Food Delivery Analytics Dashboard

A real-time food delivery analytics dashboard built with **PySpark**, **AWS S3**, and **Plotly Dash**. This project demonstrates a scalable data pipeline from data ingestion to cloud storage and interactive visualization.

---

## ğŸ“Š Project Overview

This project processes a raw food delivery dataset using **Apache Spark**, stores the cleaned data on **AWS S3**, and visualizes insights through a professional **Plotly Dash dashboard** â€” all done interactively within **Google Colab**.

---

## ğŸ”§ Tech Stack

- **PySpark** â€“ ETL and data transformation  
- **Google Colab** â€“ Development environment  
- **AWS S3** â€“ Cloud storage for cleaned data  
- **Plotly Dash** â€“ Interactive dashboard in Python  
- **ngrok** â€“ Tunnel Colab apps to the public  
- **Pandas / NumPy** â€“ Data manipulation  
- **GitHub** â€“ Version control and deployment  

---

## ğŸ› ï¸ Pipeline Architecture

```mermaid
flowchart TD
    A[Raw Dataset (CSV)] --> B[PySpark ETL in Google Colab]
    B --> C[Cleaned DataFrame]
    C --> D[Upload to AWS S3]
    C --> E[Visualized via Plotly Dash]
    D --> F[Cloud-Based Analytics Dashboard]
```

---

## âš™ï¸ Features

- âœ… Real-time ETL processing using PySpark  
- âœ… Data cleaning and transformation  
- âœ… Upload cleaned data to AWS S3  
- âœ… Build and run a Dash-based interactive dashboard  
- âœ… Run everything from Google Colab  
- âœ… GitHub-ready project structure  

---

## ğŸš€ Setup Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/upen122/Food-Delivery-Analytics-Dashboard.git
cd Food-Delivery-Analytics-Dashboard
```

### 2. Upload Your Raw Dataset

Upload your `train.csv` into Google Colab or place it in the local directory for testing.

### 3. Install Required Libraries (in Google Colab)

```python
!pip install pyspark pandas boto3 plotly dash pyngrok
```

### 4. Run the PySpark ETL Code

- Load the CSV
- Clean and transform using PySpark
- Convert to Pandas DataFrame
- Save as `cleaned_food_data.csv`

### 5. Upload Cleaned Data to AWS S3

Configure your AWS credentials in Colab:

```python
import boto3

s3 = boto3.client(
    's3',
    aws_access_key_id="YOUR_ACCESS_KEY",
    aws_secret_access_key="YOUR_SECRET_KEY"
)

s3.upload_file("cleaned_food_data.csv", "your-bucket-name", "cleaned_food_data.csv")
```

### 6. Launch Dashboard with ngrok

```python
!pip install pyngrok
from pyngrok import ngrok
public_url = ngrok.connect(8050)
print("Dashboard URL:", public_url)
```

---

## ğŸ“ Folder Structure

```
ğŸ“¦ Food-Delivery-Analytics-Dashboard
â”œâ”€â”€ ğŸ“ colab_notebooks
â”‚   â””â”€â”€ ETL_and_Dashboard.ipynb
â”œâ”€â”€ ğŸ“ data
â”‚   â”œâ”€â”€ raw
â”‚   â””â”€â”€ processed
â”œâ”€â”€ ğŸ“ dashboard
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ“¸ Screenshots

> *(Add screenshots of your dashboard here)*  
> ![dashboard-screenshot](assets/dashboard-example.png)

---

## ğŸ§‘â€ğŸ’¼ Author

**Upen**  
Data Engineering Enthusiast  
[LinkedIn]([https://www.linkedin.com/in/your-profile](https://www.linkedin.com/in/upen-singh-546999341/)) | [GitHub](https://github.com/upen122)

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸ’¡ Future Enhancements

- Add user login & authentication  
- Deploy permanently on EC2 or Render  
- Integrate real-time Kafka stream  
- Add dashboard filters and charts by cuisine, location, etc.

---

Feel free to â­ this repo if it helped you!
